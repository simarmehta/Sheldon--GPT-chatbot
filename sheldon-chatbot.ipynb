{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "     ---------------------------------------- 5.8/5.8 MB 6.5 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     -------------------------------------- 267.8/267.8 KB 8.3 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 KB ? eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "     ------------------------------------- 182.4/182.4 KB 10.8 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "     -------------------------------------- 151.6/151.6 KB 4.6 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Installing collected packages: tokenizers, typing-extensions, tqdm, regex, pyyaml, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 pyyaml-6.0 regex-2022.10.31 tokenizers-0.13.2 tqdm-4.64.1 transformers-4.25.1 typing-extensions-4.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with HuggingFace - Text Generation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reproducability\n",
    "SEED = 70\n",
    "\n",
    "#maximum number of words in output text\n",
    "MAX_LEN = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Intro\n",
    "\n",
    "** GPT-2 is capable of next word prediction on a much larger and more gigiatuas scale.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = \"Drinking a cup of coffee, always\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (22.3.1)\n",
      "Collecting jax"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement jaxlib (from versions: none)\n",
      "ERROR: No matching distribution found for jaxlib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading jax-0.3.25.tar.gz (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 5.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"http://google.com\")       \n",
    "print(r.status_code)\n",
    "\n",
    "# 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "     ------------------------------------ 266.3/266.3 MB 671.0 kB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     -------------------------------------- 895.9/895.9 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-win_amd64.whl (35 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "     -------------------------------------- 124.6/124.6 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "     ---------------------------------------- 14.2/14.2 MB 4.8 MB/s eta 0:00:00\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.1 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     -------------------------------------- 439.2/439.2 kB 9.3 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.23.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (56.0.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 5.4 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 4.2 MB/s eta 0:00:00\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.7/232.7 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "     -------------------------------------- 177.0/177.0 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.3/93.3 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.8)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-5.1.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mehta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, libclang, flatbuffers, zipp, wrapt, wheel, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, MarkupSafe, keras, h5py, grpcio, google-pasta, gast, cachetools, absl-py, werkzeug, requests-oauthlib, importlib-metadata, google-auth, astunparse, markdown, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-22.12.6 gast-0.4.0 google-auth-2.15.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.1 h5py-3.7.0 importlib-metadata-5.1.0 keras-2.11.0 libclang-14.0.6 markdown-3.4.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.28.0 termcolor-2.1.1 werkzeug-2.2.2 wheel-0.38.4 wrapt-1.14.1 zipp-3.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 1.42G/1.42G [03:07<00:00, 7.56MB/s]  \n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 354823168 \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354,823,168\n",
      "Trainable params: 354,823,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get transformers\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#get large GPT2 tokenizer and GPT2 model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "# GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id =tokenizer.eos_token_id)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#view model parameters\n",
    "GPT2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Different Decoding Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get deep learning basics\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Drinking a cup of coffee, always.\n",
      "\n",
      "I'm not sure if I'm supposed to be drinking coffee, but I'm drinking it anyway.\n",
      "\n",
      "I'm not sure if I'm supposed to be drinking coffee, but I'm drinking it anyway.\n",
      "\n",
      "I'm not sure if I'm supposed to be drinking coffee, but I'm drinking it anyway.\n",
      "\n",
      "I'm not sure if I'm supposed to be drinking coffee, but I'm drinking it anyway.\n",
      "\n",
      "I'm not sure if I'm supposed to\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search with N-Gram Penalities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Drinking a cup of coffee, always a good idea.\n",
      "\n",
      "If you're not sure what to do with your coffee cup, here's a list of things you can do:\n",
      "1: Drinking a cup of coffee, always a good idea.\n",
      "\n",
      "If you're not sure what to do with your coffee cup, here's a list of things you can do to make it better:\n",
      "2: Drinking a cup of coffee, always a good idea.\n",
      "\n",
      "If you're not sure what to do with your coffee cup, here are a few suggestions:\n",
      "3: Drinking a cup of coffee, always a good idea.\n",
      "\n",
      "If you're not sure what to do with your coffee cup, here's a list of things you can do to make it a little more interesting:\n",
      "4: Drinking a cup of coffee, always a good idea.\n",
      "\n",
      "If you're not sure what to do with your coffee cup, here's a list of things you can do to make it a little bit more interesting:\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = GPT2.generate(\n",
    "    input_ids, \n",
    "    max_length = MAX_LEN, \n",
    "    num_beams = 5, \n",
    "    no_repeat_ngram_size = 2, \n",
    "    num_return_sequences = 5, \n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "print('')\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "# now we have 3 output sequences\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Drinking a cup of coffee, always a good idea. Don't wait until you've had a lot of coffee before you eat something healthy, or you'll kick yourself for not being aware of your lack of water levels. If you don't drink enough coffee, your body won't fill it up as efficiently.\n",
      "\n",
      "Don't drink a lot of coffee unless you are doing a few runs, or your body is starving for energy. When you drink too much, you will avoid the caffeine and fat in your coffee. This will give\n"
     ]
    }
   ],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 0, \n",
    "                             temperature = 0.8\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Drinking a cup of coffee, always. What about drinking coffee alone in peace? No one is ever going to kill every morning, regardless of their size; there might be times, however, when they really should have their coffee spiked, but since the first coffee had an addictive nature, they thought they should be able to simply drink the cup and let it do its business without worrying about anything that might happen to it outside of its normal routine functioning.\n",
      "\n",
      "That being said, coffee shouldn't be forced, it's not really necessary ...\n"
     ]
    }
   ],
   "source": [
    "#sample from only top_k most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_k = 50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top-K Sampling seems to generate more sensical text than our random sampling before. But we can do even better:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P Sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Drinking a cup of coffee, always at home, when I go out to a nice restaurant is like going out for an easy meal,\" said Ms. O'Dwyer, an expert in urban settings. \"If I'm running errands, it's like eating lunch at a restaurant, and there's this pretty rush. You're totally dressed up, waiting for your turn to be at a table. That makes me think it's not so much coffee, but the place, the vibe.\"\n",
      "\n",
      "Indeed, when James W ...\n"
     ]
    }
   ],
   "source": [
    "#sample only from 80% most likely words\n",
    "sample_output = GPT2.generate(\n",
    "                             input_ids, \n",
    "                             do_sample = True, \n",
    "                             max_length = MAX_LEN, \n",
    "                             top_p = 0.8, \n",
    "                             top_k = 0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOW LETS DO SOMETHING MORE ABSURD. Lets use dynamic selection size from both top-k and top-p samplings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K and Top-P Sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Drinking a cup of coffee, always remember to drink lots of coffee.\n",
      "\n",
      "Try to do your homework every day.\n",
      "\n",
      "Don't have to worry about money if you want to buy groceries or groceries for your family.\n",
      "\n",
      "A nice place to start is with your favorite magazine or book.\n",
      "\n",
      "Get to know people and start talking.\n",
      "\n",
      "Read everything about your future.\n",
      "\n",
      "Find your passion in the present.\n",
      "\n",
      "It's a good way to start a blog.\n",
      "\n",
      "When to start your blog:\n",
      "\n",
      "If you don't own your website, or you haven't created an account yet, you can sign up for an account here. Once your account is created, you will be able to find and create posts on the site.\n",
      "\n",
      "The blog's mission is to help you achieve your personal goals and personal goals are the goal.\n",
      "\n",
      "You'll be able to find posts for various things.\n",
      "\n",
      "If you need help with any of these things, the best way to get it is to talk to other bloggers on the site...\n",
      "\n",
      "1: Drinking a cup of coffee, always make sure it's brewed without caffeine. It doesn't matter if you're on the beach or in the office. If you're drinking coffee and have a drink, drink it quickly.\n",
      "\n",
      "What Is Coffee?\n",
      "\n",
      "Coffee is brewed with water, usually by a process called roasting, which is the process where you brew coffee in a controlled environment. There are several coffee roasters who are considered coffee-makers, and if you look around online, you'll find a myriad of options to choose from. If you're not sure what to call the coffee you're brewing, use it just like any other drink — and don't put it in a cup.\n",
      "\n",
      "In addition to coffee, coffee also has other uses, including tea and sugar, and many are sold in convenience stores and in grocery stores. But if you have an interest in becoming a coffee-maker, you should be familiar with the basics of what coffee does. If you've never brewed coffee before, you may find the information you're looking for...\n",
      "\n",
      "2: Drinking a cup of coffee, always in a coffee mug, never with the cap off is more dangerous than drinking a glass of water, but drinking in a cup always makes sense, especially if the cup is not a full glass, the water is in a reusable cup, or both.\n",
      "\n",
      "2. The cup should always be in a container that is designed to hold at least two ounces.\n",
      "\n",
      "The cups most often found for water are disposable water mugs and are the best option.\n",
      "\n",
      "3. The cup should never be in a cup that is completely flat.\n",
      "\n",
      "Flat or raised cups may actually cause your cup to shatter and/or lose its shape. The safest way to avoid this is to never use a flat cup that is filled with water.\n",
      "\n",
      "4. The cup should never be in a cup that is covered in lumps or crumbs.\n",
      "\n",
      "Lumps or crumbs can cause injury to your eyes, nose, or mouth if you pour enough of the coffee on them. You should only pour water on a flat cup...\n",
      "\n",
      "3: Drinking a cup of coffee, always on the go, I found myself thinking about what I might be doing if I weren't in school. How would I cope if I could go to work without having to get up to the front desk? I couldn't stop thinking about this. So I stopped. I didn't want to get up the next day and start looking for a new job, even though I knew I would need one. So I stopped. The answer to that question was: No.\n",
      "\n",
      "As a result, I felt like my life was a living hell: I was stressed, my friends and family were being mean to me, I was stressed, I had a job to do that I didn't feel I could manage. I was going through something I wouldn't feel like living through if I were going to get a job. And it was this, but more: I wasn't having sex, I didn't know how to handle myself if I was going out and drinking, I couldn't manage the pressure that being on time and getting the job that...\n",
      "\n",
      "4: Drinking a cup of coffee, always, whenever you want.'\n",
      "\n",
      "'You don't need to ask me. Just get it over with.'\n",
      "\n",
      "I could only stare at her and wait for her to speak again.\n",
      "\n",
      "'You're doing it again. Why are you doing it?'\n",
      "\n",
      "I felt a wave of pain. It felt like I had been slapped or stabbed. I felt something that shouldn't have happened in my mouth. I had been touched by the world in a way that I could not normally do, and I could not be sure that it had happened in the same way. But it had.\n",
      "\n",
      "'This is the power of your mind. You should be proud.'\n",
      "\n",
      "I was too stunned to say anything and I just felt numb, lost. My body wanted to move and I could not see why. It was too cold. It was like I was sitting in a glass. The water was still and I could barely see my feet and my body was too tired to move.\n",
      "\n",
      "'The power of the mind...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine both sampling techniques\n",
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .7,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85, \n",
    "                              num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'In a shocking finding, scientist discovered a herd of monsters with three gentials living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the monsters spoke perfect English.'\n",
    "\n",
    "input_ids = tokenizer.encode(prompt1, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sample_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                              \u001b[49m\u001b[38;5;66;43;03m#to test how long we can generate and it be coherent\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;66;43;03m#temperature = .8,\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.85\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;66;43;03m#num_return_sequences = 5\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_outputs):\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\tf_utils.py:694\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[1;34m(self, input_ids, max_length, max_new_tokens, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39mif\u001b[39;00m do_sample \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m num_beams \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    693\u001b[0m     seed \u001b[39m=\u001b[39m model_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 694\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    695\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    696\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m    697\u001b[0m         max_new_tokens\u001b[39m=\u001b[39mmax_new_tokens,\n\u001b[0;32m    698\u001b[0m         min_length\u001b[39m=\u001b[39mmin_length,\n\u001b[0;32m    699\u001b[0m         do_sample\u001b[39m=\u001b[39mdo_sample,\n\u001b[0;32m    700\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[0;32m    701\u001b[0m         num_beams\u001b[39m=\u001b[39mnum_beams,\n\u001b[0;32m    702\u001b[0m         temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[0;32m    703\u001b[0m         penalty_alpha\u001b[39m=\u001b[39mpenalty_alpha,\n\u001b[0;32m    704\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[0;32m    705\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m    706\u001b[0m         repetition_penalty\u001b[39m=\u001b[39mrepetition_penalty,\n\u001b[0;32m    707\u001b[0m         bad_words_ids\u001b[39m=\u001b[39mbad_words_ids,\n\u001b[0;32m    708\u001b[0m         bos_token_id\u001b[39m=\u001b[39mbos_token_id,\n\u001b[0;32m    709\u001b[0m         pad_token_id\u001b[39m=\u001b[39mpad_token_id,\n\u001b[0;32m    710\u001b[0m         eos_token_id\u001b[39m=\u001b[39meos_token_id,\n\u001b[0;32m    711\u001b[0m         length_penalty\u001b[39m=\u001b[39mlength_penalty,\n\u001b[0;32m    712\u001b[0m         no_repeat_ngram_size\u001b[39m=\u001b[39mno_repeat_ngram_size,\n\u001b[0;32m    713\u001b[0m         num_return_sequences\u001b[39m=\u001b[39mnum_return_sequences,\n\u001b[0;32m    714\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    715\u001b[0m         decoder_start_token_id\u001b[39m=\u001b[39mdecoder_start_token_id,\n\u001b[0;32m    716\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    717\u001b[0m         seed\u001b[39m=\u001b[39mseed,\n\u001b[0;32m    718\u001b[0m         output_scores\u001b[39m=\u001b[39moutput_scores,\n\u001b[0;32m    719\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    720\u001b[0m         output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    721\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m    722\u001b[0m         forced_bos_token_id\u001b[39m=\u001b[39mforced_bos_token_id,\n\u001b[0;32m    723\u001b[0m         forced_eos_token_id\u001b[39m=\u001b[39mforced_eos_token_id,\n\u001b[0;32m    724\u001b[0m         suppress_tokens\u001b[39m=\u001b[39msuppress_tokens,\n\u001b[0;32m    725\u001b[0m         begin_suppress_tokens\u001b[39m=\u001b[39mbegin_suppress_tokens,\n\u001b[0;32m    726\u001b[0m         forced_decoder_ids\u001b[39m=\u001b[39mforced_decoder_ids,\n\u001b[0;32m    727\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    728\u001b[0m     )\n\u001b[0;32m    730\u001b[0m \u001b[39m# We cannot generate if the model does not have a LM head\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_output_embeddings() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\tf_utils.py:1906\u001b[0m, in \u001b[0;36mTFGenerationMixin._generate\u001b[1;34m(self, input_ids, max_length, max_new_tokens, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, seed, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1898\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1899\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1900\u001b[0m         expand_size\u001b[39m=\u001b[39mnum_return_sequences,\n\u001b[0;32m   1901\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1902\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1903\u001b[0m     )\n\u001b[0;32m   1905\u001b[0m     \u001b[39m# 12. run sample\u001b[39;00m\n\u001b[1;32m-> 1906\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[0;32m   1907\u001b[0m         input_ids,\n\u001b[0;32m   1908\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1909\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[0;32m   1910\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   1911\u001b[0m         pad_token_id\u001b[39m=\u001b[39mpad_token_id,\n\u001b[0;32m   1912\u001b[0m         eos_token_id\u001b[39m=\u001b[39meos_token_id,\n\u001b[0;32m   1913\u001b[0m         seed\u001b[39m=\u001b[39mseed,\n\u001b[0;32m   1914\u001b[0m         output_scores\u001b[39m=\u001b[39moutput_scores,\n\u001b[0;32m   1915\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1916\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1917\u001b[0m     )\n\u001b[0;32m   1919\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[0;32m   1920\u001b[0m     \u001b[39mif\u001b[39;00m num_beams \u001b[39m<\u001b[39m num_return_sequences:\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\tf_utils.py:2809\u001b[0m, in \u001b[0;36mTFGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, logits_warper, max_length, pad_token_id, eos_token_id, seed, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[0;32m   2807\u001b[0m \u001b[39mif\u001b[39;00m sample_cond_fn(generated, finished_sequences, cur_len, model_kwargs):\n\u001b[0;32m   2808\u001b[0m     maximum_iterations \u001b[39m=\u001b[39m max_length \u001b[39m-\u001b[39m cur_len\n\u001b[1;32m-> 2809\u001b[0m     generated, _, cur_len, _ \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mwhile_loop(\n\u001b[0;32m   2810\u001b[0m         sample_cond_fn,\n\u001b[0;32m   2811\u001b[0m         sample_body_fn,\n\u001b[0;32m   2812\u001b[0m         (generated, finished_sequences, cur_len, model_kwargs),\n\u001b[0;32m   2813\u001b[0m         maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[0;32m   2814\u001b[0m     )\n\u001b[0;32m   2816\u001b[0m \u001b[39m# 6. prepare outputs\u001b[39;00m\n\u001b[0;32m   2817\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_xla:\n\u001b[0;32m   2818\u001b[0m     \u001b[39m# cut for backward compatibility\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    623\u001b[0m         logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    624\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    625\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mwill be removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m    626\u001b[0m             _call_location(), decorator_utils\u001b[39m.\u001b[39mget_qualified_name(func),\n\u001b[0;32m    627\u001b[0m             func\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m, arg_name, arg_value, \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    628\u001b[0m             \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date), instructions)\n\u001b[1;32m--> 629\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2516\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[0;32m   2340\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwhile_loop\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m   2341\u001b[0m \u001b[39m@deprecation\u001b[39m\u001b[39m.\u001b[39mdeprecated_arg_values(\n\u001b[0;32m   2342\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2357\u001b[0m                   maximum_iterations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2358\u001b[0m                   name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2359\u001b[0m   \u001b[39m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[0;32m   2360\u001b[0m \n\u001b[0;32m   2361\u001b[0m \u001b[39m  `cond` is a callable returning a boolean scalar tensor. `body` is a callable\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2514\u001b[0m \n\u001b[0;32m   2515\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2516\u001b[0m   \u001b[39mreturn\u001b[39;00m while_loop(\n\u001b[0;32m   2517\u001b[0m       cond\u001b[39m=\u001b[39;49mcond,\n\u001b[0;32m   2518\u001b[0m       body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m   2519\u001b[0m       loop_vars\u001b[39m=\u001b[39;49mloop_vars,\n\u001b[0;32m   2520\u001b[0m       shape_invariants\u001b[39m=\u001b[39;49mshape_invariants,\n\u001b[0;32m   2521\u001b[0m       parallel_iterations\u001b[39m=\u001b[39;49mparallel_iterations,\n\u001b[0;32m   2522\u001b[0m       back_prop\u001b[39m=\u001b[39;49mback_prop,\n\u001b[0;32m   2523\u001b[0m       swap_memory\u001b[39m=\u001b[39;49mswap_memory,\n\u001b[0;32m   2524\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   2525\u001b[0m       maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[0;32m   2526\u001b[0m       return_same_structure\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2765\u001b[0m, in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2762\u001b[0m loop_var_structure \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(type_spec\u001b[39m.\u001b[39mtype_spec_from_value,\n\u001b[0;32m   2763\u001b[0m                                         \u001b[39mlist\u001b[39m(loop_vars))\n\u001b[0;32m   2764\u001b[0m \u001b[39mwhile\u001b[39;00m cond(\u001b[39m*\u001b[39mloop_vars):\n\u001b[1;32m-> 2765\u001b[0m   loop_vars \u001b[39m=\u001b[39m body(\u001b[39m*\u001b[39;49mloop_vars)\n\u001b[0;32m   2766\u001b[0m   \u001b[39mif\u001b[39;00m try_to_pack \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(loop_vars, (\u001b[39mlist\u001b[39m, _basetuple)):\n\u001b[0;32m   2767\u001b[0m     packed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2756\u001b[0m, in \u001b[0;36mwhile_loop.<locals>.<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   2753\u001b[0m     loop_vars \u001b[39m=\u001b[39m (counter, loop_vars)\n\u001b[0;32m   2754\u001b[0m     cond \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i, lv: (  \u001b[39m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[0;32m   2755\u001b[0m         math_ops\u001b[39m.\u001b[39mlogical_and(i \u001b[39m<\u001b[39m maximum_iterations, orig_cond(\u001b[39m*\u001b[39mlv)))\n\u001b[1;32m-> 2756\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i, lv: (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, orig_body(\u001b[39m*\u001b[39;49mlv))\n\u001b[0;32m   2757\u001b[0m   try_to_pack \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   2759\u001b[0m \u001b[39mif\u001b[39;00m executing_eagerly:\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\tf_utils.py:2725\u001b[0m, in \u001b[0;36mTFGenerationMixin.sample.<locals>.sample_body_fn\u001b[1;34m(generated, finished_sequences, cur_len, model_kwargs)\u001b[0m\n\u001b[0;32m   2723\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2724\u001b[0m \u001b[39m# forward pass to get next token logits\u001b[39;00m\n\u001b[1;32m-> 2725\u001b[0m model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2726\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2727\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2728\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2729\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2730\u001b[0m )\n\u001b[0;32m   2731\u001b[0m next_token_logits \u001b[39m=\u001b[39m model_outputs\u001b[39m.\u001b[39mlogits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   2733\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    559\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1131\u001b[0m ):\n\u001b[1;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_tf_utils.py:432\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    431\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 432\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py:923\u001b[0m, in \u001b[0;36mTFGPT2LMHeadModel.call\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m    906\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[0;32m    907\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    908\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    920\u001b[0m     training\u001b[39m=\u001b[39mtraining,\n\u001b[0;32m    921\u001b[0m )\n\u001b[0;32m    922\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 923\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mwte(hidden_states, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    925\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    927\u001b[0m     \u001b[39m# shift labels to the left and cut last logit token\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1131\u001b[0m ):\n\u001b[1;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_tf_utils.py:3064\u001b[0m, in \u001b[0;36mTFSharedEmbeddings.call\u001b[1;34m(self, inputs, mode)\u001b[0m\n\u001b[0;32m   3062\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding(inputs)\n\u001b[0;32m   3063\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 3064\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_linear(inputs)\n\u001b[0;32m   3065\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3066\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmode \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m is not valid.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_tf_utils.py:3084\u001b[0m, in \u001b[0;36mTFSharedEmbeddings._linear\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3082\u001b[0m first_dims \u001b[39m=\u001b[39m shape_list(inputs)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   3083\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(inputs, [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size])\n\u001b[1;32m-> 3084\u001b[0m logits \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mmatmul(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, transpose_b\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   3086\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mreshape(logits, first_dims \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size])\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3714\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[0;32m   3711\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39mbatch_mat_mul_v3(\n\u001b[0;32m   3712\u001b[0m       a, b, adj_x\u001b[39m=\u001b[39madjoint_a, adj_y\u001b[39m=\u001b[39madjoint_b, Tout\u001b[39m=\u001b[39moutput_type, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   3713\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3714\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(\n\u001b[0;32m   3715\u001b[0m       a, b, transpose_a\u001b[39m=\u001b[39;49mtranspose_a, transpose_b\u001b[39m=\u001b[39;49mtranspose_b, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\mehta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6713\u001b[0m, in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6711\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   6712\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 6713\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   6714\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   6715\u001b[0m       transpose_b)\n\u001b[0;32m   6716\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   6717\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WELL, ill use bernard to do my friend's desertation project on Cross-cultural management helps understanding the intricasies of business**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"Cross-cultural management helps understanding the intricasies of business\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt2, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Cross-cultural management helps understanding the intricasies of business, but it doesn't do it alone. It doesn't tell you how to solve problems, or how to build new ones. The same is true for the concept of intercultural communication. It's not easy to learn to communicate in different cultures and cultures, and that's just the way it is.\n",
      "\n",
      "One way I've come to understand that is by trying to be more comfortable with my Spanish and having conversations with people in my culture, particularly with people who are also bilingual. For me, the Spanish is like the Italian to English: I can't speak either of them fluently, but I can understand both.\n",
      "\n",
      "As a result, I find it very easy...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_outputs = GPT2.generate(\n",
    "                              input_ids,\n",
    "                              do_sample = True, \n",
    "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
    "                              #temperature = .8,\n",
    "                              top_k = 50, \n",
    "                              top_p = 0.85 \n",
    "                              #num_return_sequences = 5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "39d7225d0519abb9400afb483e9ccdd16553ce5ea48442cf97b7fe4dcaaa2e37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
